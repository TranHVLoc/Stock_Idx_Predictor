{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCbxzIlmjCaH"
      },
      "source": [
        "### HOW TO RUN: ###\n",
        "\n",
        "\n",
        "* Navigate to the folder icon on the left\n",
        "* Click on the upload file icon\n",
        "* Upload agg.csv, conf.ini, testloss.txt, and best-model.keras from the github directory\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Already install\n",
        "!pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnDW1jbHrEoQ",
        "outputId": "d4684aed-66ff-4c78-e94e-77687cf2f168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting keras>=3.2.0 (from scikeras)\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn>=1.4.2 (from scikeras)\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.7.1)\n",
            "Collecting namex (from keras>=3.2.0->scikeras)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.9.0)\n",
            "Collecting optree (from keras>=3.2.0->scikeras)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Installing collected packages: namex, optree, scikit-learn, keras, scikeras\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.3.3 namex-0.0.8 optree-0.11.0 scikeras-0.13.0 scikit-learn-1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai2nk3Cgyfys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d68280b2-c863-4d90-e94b-e35b70aa361e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import configparser\n",
        "import seaborn as sns\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO5FPmodgtEs"
      },
      "source": [
        "### TODOS: ###\n",
        "\n",
        "\n",
        "*  Data preprocessing, visualize the relationship and choose the best ones (IN PROGRESS)\n",
        "*  Show a graph that loss is reduced through training (DONE)\n",
        "*  Add a metric for the model (DONE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkVdGlZo1mbf"
      },
      "outputs": [],
      "source": [
        "def dropUnneeded(df, drop_non_correlated=False):\n",
        "    try:\n",
        "        df = df.drop(\"S&P OPEN\", axis=1)\n",
        "        df = df.drop(\"S&P HIGH\", axis=1)\n",
        "        df = df.drop(\"S&P LOW\", axis=1)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if drop_non_correlated:\n",
        "        # List of non-correlated features to be dropped.\n",
        "        try:\n",
        "            non_correlated_features = ['1 Year T', '2 Year T', '3 Year T', '5 Year T', '7 Year T', '10 Year T', '30 Year T',\n",
        "                                      'Mortgage 15 Yr',\n",
        "                                      #  'Mortgage 30 Yr',\n",
        "                                      'FFR',\n",
        "                                      'UNEMP',\n",
        "                                      'PRIME RATE',\n",
        "                                      'UMich Consumer Sentiment'\n",
        "                                      ]\n",
        "            '''\n",
        "            # Threshold for dropping non-correlated features\n",
        "            threshold = 0.1  # or some other value close to 0\n",
        "\n",
        "            # Identify non-correlated features\n",
        "            non_correlated_features = corr_matrix.index[abs(corr_matrix['S&P CLOSE']) < threshold].tolist()\n",
        "            '''\n",
        "            # Drop the non-correlated features from the dataframe.\n",
        "            df = df.drop(non_correlated_features, axis=1)\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "     # Convert to numpy\n",
        "    data = df.to_numpy()\n",
        "\n",
        "    # Format the data to a 3D shape so the LSTM can use it.\n",
        "    # The resulting shapes will be as follows:\n",
        "    #    dataX: (number_of_input_series, number_of_timesteps, number_of_features)\n",
        "    #    dataY: (number_of_input_series, 1)\n",
        "    dataX = []\n",
        "    for i in range(n_past, len(data) - n_future + 1):\n",
        "      dataX.append(data[i - int(conf['data']['n_past']):i, 0:data.shape[1]])\n",
        "\n",
        "    return (df, np.array(dataX))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb2KfOOfy34r"
      },
      "outputs": [],
      "source": [
        "# Returns a tuple of (df, dataX, dataY)\n",
        "# Where df is the original dataframe, dataX is the formatted normalized input data,\n",
        "# and dataY is the formatted normalized output data.\n",
        "# The dataframe will not include the date column if df_dates is False.\n",
        "def preprocess(n_past, n_future, label_column, null, df_dates=True, include_month_change=False, remove_label_from_input=False):\n",
        "    # Load the data.\n",
        "    # We'll forward fill any missing values, so long as there are no more than 5 of them consecutively,\n",
        "    # then fill any remaining missing values with our null constant.\n",
        "    df = pd.read_csv(\"./agg.csv\").ffill(limit=5).fillna(null)\n",
        "\n",
        "    if (not include_month_change and 'S&P MONTH CHANGE' in df.columns):\n",
        "        df = df.drop('S&P MONTH CHANGE', axis=1)\n",
        "\n",
        "    # Remove date column temporarily; not useful for normalizing\n",
        "    ddf = df.iloc[:, 1:]\n",
        "\n",
        "    # Mean normalize (standardization)\n",
        "    mean = ddf.mean()\n",
        "    std = ddf.std()\n",
        "    ddf = (ddf - mean) / std\n",
        "\n",
        "    # Convert to numpy\n",
        "    data = ddf.to_numpy()\n",
        "\n",
        "    # Format the data to a 3D shape so the LSTM can use it.\n",
        "    # The resulting shapes will be as follows:\n",
        "    #    dataX: (number_of_input_series, number_of_timesteps, number_of_features)\n",
        "    #    dataY: (number_of_input_series, 1)\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "\n",
        "    for i in range(n_past, len(data) - n_future + 1):\n",
        "        dataX.append(data[i - n_past:i, 0:data.shape[1]])\n",
        "        dataY.append(data[i + n_future - 1:i + n_future, label_column])\n",
        "\n",
        "    # Remove date column permanently if requested\n",
        "    if not df_dates:\n",
        "        df = df.iloc[:, 1:]\n",
        "\n",
        "    # Delete S&P change, price open and price close columns from input data.\n",
        "    if remove_label_from_input:\n",
        "        dataX = np.delete(dataX, label_column, axis=2)\n",
        "\n",
        "    # Return a tuple containing (df, dataX, dataY)\n",
        "    return (df,np.array(dataX),np.array(dataY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2SDQHXOjf9l"
      },
      "source": [
        "## Config the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd4IceaLy80_"
      },
      "outputs": [],
      "source": [
        "conf = configparser.ConfigParser()\n",
        "conf.read(\"./conf.ini\")\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "LEARNING_RATE = 0.003#0.005\n",
        "EPOCHS = 2\n",
        "WEIGHT_DECAY = 0.002#0.002\n",
        "ACTIVATION = \"tanh\"\n",
        "L1_REG = 0.001;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6la3Y3mVzKtx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "b852d32d-9449-4bb4-90f7-82e23c22a89c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f91f98de380f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The arbitrary null value for missing data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnull\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'null'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# How many timesteps in the future we're predicting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_future\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_future'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/configparser.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_section\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_section\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proxies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'data'"
          ]
        }
      ],
      "source": [
        "# The arbitrary null value for missing data.\n",
        "null = int(conf['data']['null'])\n",
        "\n",
        "# How many timesteps in the future we're predicting\n",
        "n_future = int(conf['data']['n_future'])\n",
        "# The size of each input series (in timesteps).\n",
        "n_past = int(conf['data']['n_past'])\n",
        "# The column index containing the output, AFTER removing the date column.\n",
        "label_column = int(conf['data']['label_column'])\n",
        "# Include the S&P percentage change (by about a month)\n",
        "include_month_change = conf['data'].getboolean('include_month_change')\n",
        "# Remove the label from the features\n",
        "remove_label_from_input = conf['data'].getboolean('remove_label_from_input')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_Cx6gl8zWYc"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "df, dataX, dataY = preprocess(n_past, n_future, label_column, null, df_dates=False, include_month_change=include_month_change, remove_label_from_input=remove_label_from_input)\n",
        "\n",
        "print(df.head())\n",
        "print(\"Shape:\", dataX.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqDBhsL2usw3"
      },
      "outputs": [],
      "source": [
        "corr_matrix = df.corr(method=\"kendall\")\n",
        "plt.figure(figsize=(20,20))\n",
        "sns.heatmap(corr_matrix, linewidths=2, xticklabels=True, yticklabels=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIH09r81zN6m"
      },
      "outputs": [],
      "source": [
        "label_correlations = corr_matrix['S&P CLOSE'].sort_values(ascending=False)\n",
        "# label_matrix = corr_matrix[['S&P CLOSE']]\n",
        "plt.figure(figsize=(10, 10))\n",
        "# sns.heatmap(label_matrix, linewidths=2, xticklabels=True, yticklabels=True)\n",
        "sns.heatmap(label_correlations.to_frame(), annot=True, fmt='.2f', center=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QPJlUgIy51Y"
      },
      "source": [
        "We can see from the Kendall correlation matrix that a few of our columns have a low correlation with our label (S&P Close). These include but are not limited to: VIX levels, 15 year mortgage rates, consumer sentiment, and investor sentiment.\n",
        "\n",
        "We used Kendall Correlation due to its reputation of being a good coefficient for nonlinear data. While it isn't perfect, it's a good starting point for feature selection."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the weak features\n",
        "# df, dataX = dropUnneeded(df, drop_non_correlated=True)"
      ],
      "metadata": {
        "id": "td_Yoxmh3piL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cy6EMbBzhEJ"
      },
      "outputs": [],
      "source": [
        "mean = df.mean()\n",
        "std = df.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DqvvViZzl-O"
      },
      "outputs": [],
      "source": [
        "# Train test split\n",
        "trainX, testX, trainY, testY = train_test_split(dataX, dataY, test_size=0.05, shuffle=False)\n",
        "\n",
        "# Shuffle only the training data\n",
        "trainRandomIndices = np.random.permutation(len(trainX))\n",
        "trainX = trainX[trainRandomIndices]\n",
        "trainY = trainY[trainRandomIndices]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SHAPE = (dataX.shape[1],dataX.shape[2])\n",
        "\n",
        "def build_model(activation):\n",
        "\n",
        "    inputs = keras.Input(shape=SHAPE, dtype=\"float32\", name=\"inputs\")\n",
        "    x = layers.LSTM(25, return_sequences=True, activation=activation, kernel_regularizer=regularizers.L1(L1_REG))(inputs)\n",
        "    # x = layers.LSTM(40, return_sequences=True, activation=ACTIVATION)(x)\n",
        "    x = layers.LSTM(25, return_sequences=False, activation=activation)(x)\n",
        "\n",
        "    outputs = layers.Dense(1, activation=activation, name=\"outputs\")(x) # activation=\"tanh\",\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss=\"mse\", run_eagerly=False, optimizer=(tf.keras.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)), metrics=[keras.metrics.RootMeanSquaredError(), keras.losses.MeanAbsoluteError()])\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "rEXaRaIOnE0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybeMnPYLznrg"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = build_model(activation=ACTIVATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAOKtD5sXr2Z"
      },
      "outputs": [],
      "source": [
        "# visualize the model\n",
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model_plot.png',show_layer_activations=True, show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZh8LQ6azs9o"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# from keras.callbacks import History\n",
        "\n",
        "# model = KerasRegressor(build_fn=build_model, verbose=0, activation=\"tanh\")\n",
        "# # Define the grid of hyperparameters to search\n",
        "# param_dist = {\n",
        "#   'activation': ['tanh', 'sigmoid'],\n",
        "#   'optimizer': ['adam'],\n",
        "#   'batch_size': [2, 4, 1],\n",
        "#   'epochs': [4, 3, 2]\n",
        "# }\n",
        "\n",
        "# history = History()\n",
        "# # Create a RandomizedSearchCV object\n",
        "# random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3, verbose=2)\n",
        "# random_search_results = random_search.fit(trainX, trainY, callbacks=[history])\n",
        "\n",
        "# print(\"Best: %f using %s\" % (random_search_results.best_score_, random_search_results.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "\n",
        "# with open('random_search_cv_results.pkl', 'wb') as f:\n",
        "#     pickle.dump(random_search_results, f)"
      ],
      "metadata": {
        "id": "QZ55n7kbCO76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('random_search_cv_results.pkl', 'rb') as f:\n",
        "#     results = pickle.load(f)"
      ],
      "metadata": {
        "id": "cr2qgxyqDJI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result_df = pd.DataFrame(results.cv_results_)\n",
        "# result_df = result_df.drop([\"param_batch_size\", \"param_optimizer\", \"param_epochs\", \"param_batch_size\", \"param_activation\",\n",
        "#                             \"split0_test_score\", \"split1_test_score\", \"split2_test_score\", \"mean_fit_time\", \"mean_score_time\", \"std_fit_time\", \"std_score_time\"], axis=1)\n",
        "# result_df"
      ],
      "metadata": {
        "id": "AP57f_X2DLXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_activation = random_search_results.best_params_['activation']\n",
        "# best_optimizer = random_search_results.best_params_['optimizer']\n",
        "# best_batch_size = random_search_results.best_params_['batch_size']\n",
        "# best_epochs = random_search_results.best_params_['epochs']\n",
        "\n",
        "# print(best_batch_size)\n",
        "# best_model = build_model(best_activation)\n",
        "\n",
        "# history = best_model.fit(trainX, trainY, epochs=5, batch_size=best_batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "W5F-TNN-ULCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptJkxBvYHAmf"
      },
      "outputs": [],
      "source": [
        "# plot the loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gcti9-hMHsQu"
      },
      "outputs": [],
      "source": [
        "# Plot training & validation RSME\n",
        "plt.plot(history.history['root_mean_squared_error'], label='Train RMSE')\n",
        "plt.title('Model RMSE')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OwRHuNxH08U"
      },
      "outputs": [],
      "source": [
        "# plot mae\n",
        "plt.plot(history.history['mean_absolute_error'], label='Train MAE')\n",
        "plt.title('Model MAE')\n",
        "plt.ylabel('MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6P1_oXEz0eG"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data\n",
        "train_loss = model.evaluate(trainX, trainY, verbose=0)\n",
        "test_loss = model.evaluate(testX, testY, verbose=0)\n",
        "\n",
        "print('Train Loss: ', train_loss)\n",
        "print('Test Loss: ', test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnlFauortUeX"
      },
      "source": [
        "## Compare current Loss and Model performance with previously trained model to update bessloss.txt file and best-model.keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKuR4_WGz4cs"
      },
      "outputs": [],
      "source": [
        "bestlossr = open('bestloss.txt', 'r')\n",
        "bestloss = bestlossr.read()\n",
        "\n",
        "# Save the model if this is a new best loss ([0] = MSE)\n",
        "if (not bestloss.replace(\".\", \"\").isnumeric()) or test_loss[0] < float(bestloss):\n",
        "\tprint(\"New best loss! Saving model...\")\n",
        "\tmodel.save(\"./best-model.keras\")\n",
        "\n",
        "\tbestlossw = open('bestloss.txt', 'w')\n",
        "\tbestlossw.write(str(test_loss[0]))\n",
        "\tbestlossw.close()\n",
        "\n",
        "\tmeanstd = open('meanstd.txt', 'w')\n",
        "\tmeanstd.write(str(mean.to_list()) + \"\\n\" + str(std.to_list()))\n",
        "\tmeanstd.close()\n",
        "\n",
        "else: print(\"Model not saved, best lost still\", bestloss)\n",
        "\n",
        "bestlossr.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avfzYxvhz8ib"
      },
      "outputs": [],
      "source": [
        "# Plot the predicted graph movement\n",
        "testYReg = testY * std[df.columns[label_column]] + mean[df.columns[label_column]]\n",
        "testYHat = model.predict(testX) * std[df.columns[label_column]] + mean[df.columns[label_column]]\n",
        "\n",
        "print(testYReg.shape)\n",
        "print(testYHat.shape)\n",
        "\n",
        "for i in range(0, len(testYHat)):\n",
        "\tprint(\"Predicted: \", testYHat[i], \"Actual: \", testYReg[i])\n",
        "\n",
        "print(\"Train loss:\", train_loss)\n",
        "print(\"Test loss:\", test_loss)\n",
        "\n",
        "plt.plot(testYReg, c=\"blue\", label=\"Actual\")\n",
        "plt.plot(testYHat, c=\"red\", label=\"Predicted\")\n",
        "\n",
        "plt.ylabel(\"S&P Index Level\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UbQfubQhbSY"
      },
      "outputs": [],
      "source": [
        "# Metrics\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    # Convert inputs to numpy arrays for vectorized operation\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "\n",
        "    # Avoid division by zero and only use non-zero elements for calculation\n",
        "    mask = y_true != 0\n",
        "    y_true = y_true[mask]\n",
        "    y_pred = y_pred[mask]\n",
        "\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "    return mape\n",
        "\n",
        "mse = mean_squared_error(testYReg, testYHat)\n",
        "mape = mean_absolute_percentage_error(testYReg, testYHat)\n",
        "print(\"Root Mean Squared Error (MSE):\", math.sqrt(mse))\n",
        "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uyd2wmjMddex"
      },
      "outputs": [],
      "source": [
        "with open('log.csv', 'a') as logfile:\n",
        "  # trainXLength, trainXTimesteps, trainXFeatures, Batch Size, Learning Rate, Epochs, Weight Decay, L1 Reg, Activation, layers, train MSE, test MSE, test RMSE, test MAE, test MAPE\n",
        "  logfile.write(f\"{trainX.shape[0]},{trainX.shape[1]},{trainX.shape[2]},{BATCH_SIZE},{LEARNING_RATE},{EPOCHS},{WEIGHT_DECAY},{L1_REG},{ACTIVATION},{len(model.layers)},\")\n",
        "  logfile.write(f\"{train_loss[0]},{test_loss[0]},{test_loss[1]},{test_loss[2]},{mape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QpXnPVoiAXe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}